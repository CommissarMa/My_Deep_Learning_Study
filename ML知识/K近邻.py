#%% [markdown]
# # K近邻
# **Author**: `CommissarMa <https://github.com/commissarma>`
#
# =======================================================
# ## 概述
# K近邻是一种基本的分类与回归方法，它的输入为实例的特征向量，
# 通过计算新数据与训练数据特征值之间的距离，然后选取K（K>=1）个距离
# 最近的邻居进行分类判断（投票法）或者回归。如果K=1，那么新数据被简
# 单地分配给其近邻的类。  
# 
# 对于分类问题：输出为实例的类别。分类时，对于新的实例，根据其k个最
# 近邻的训练实例的类别，通过多数表决等方式进行预测。
#
# 对于回归问题：输出为实例的值。回归时，对于新的实例，取其k个最近邻
# 的训练实例的平均值为预测值。
# 
# 注意：K近邻不具有现实的学习过程，它是直接预测。
#
# ## KNN的三要素
# k值选择，距离度量、分类决策规则
# + k值选择：当k=1时的k近邻算法称为最近邻算法。注意：k值的选择会对
# k近邻的结果产生重大影响。
# 
# 若k值较小，则相当于用较小的领域中的训练实例进行预测，“学习”的近似
# 误差较小。优点：只有较近的训练实例能起作用。缺点：“学习”的估计误差
# 会增大，预测结果会对近邻的实例点非常敏感。若近邻点正好是噪声，则
# 预测会出错。即k值的减小意味着模型整体变复杂，易发生过拟合。
#
# 若k值过大，则相当于用较大的邻域中的训练实例进行预测。优点：减少学习
# 的估计误差。缺点：学习的近似误差变大。这时输入实例较远的训练实例也
# 会对预测起作用，使预测发生错误，即k值增大意味着模型整体变简单。当k=N
# 时，无论输入实例是什么，都将它预测为训练实例中最多的类。此时模型过于
# 简单，完全忽略了训练实例中大量有用的信息。
#
# 应用中，k值一般取一个较小的数值。通常采用交叉验证法来选取最优的k值，
# 
# + 距离度量：knn算法要求数据的所有特征都可做量化的比较。若特征中存在
# 非数值的类型，比如颜色（红，黑，蓝），那么我们可以转换为灰度值进行
# 比较。KNN中一般使用欧式距离作为特征空间的距离，也可以是$L_p$距离。
# 在文本分类中，我们可以使用汉明距离。
#
# + 分类决策规则：通常采用多数表决，也可以基于距离的远近进行加权投票，
# 距离越近的样本权重越大。
#
# ## KD树
# 一般来说，我们通过线性扫描找出k近邻的实例，但该方法计算非常耗时。
# kd树可以大幅提高k近邻搜索的效率。
# kd树是二叉树，平均复杂度为$O(logN)$，N为训练集大小。kd树适合N>>k
# 的情形，当N与维度k接近时，搜索的效率接近线性扫描。

#%% [markdown]
